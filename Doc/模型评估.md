# 模型评估

## 经验误差与过拟合

- **错误率** (error rate)：分类错误的样本数a占样本总数m的比例$\frac{a}{m}$;
- **精度** (accuracy)：$1-\frac{a}{m}$;

学习器的实际预测输出与样本的真实输出之间的差异称为误差 (error)；在训练集上的误差称为*“训练误差”*（training error）或*“经验误差”*；在新样本上的误差称为*“泛化误差”*（generalization error）。显然，我们想的到的是**泛化误差小**的学习器，但实际能做的是努力使**经验误差最小化**。

- **过拟合**（overfitting）：我们实际希望的，是在新样本上能表现得很好的学习器。为了达到这个 目的，应该从训练样本中尽可能学出适用于所有潜在样本的"普遍规律"，这 样才能在遇到新样本时做出正确的判别。然而，当学习器把训练样本学得"太好"了的时候，很可能巳经把**训练样本自身的一些特点**当作了所有潜在样本都会具有的一般性质，这样就会导致**泛化性能下降**。
- **欠拟合**（underfitting）：对训练样本的一般性质尚未学好。

有多种因素可能导致过拟合，其中最常见的就是学习能力过于强大。而学习能力是否“过于强大”，是由学习算法和数据内涵共同决定的。而欠拟合通常是由学习能力低下造成的。**过拟合是无法彻底避免的**，所能做的只是“缓解”，或者说减小期风险。

## 评估方法

通常，使用一个*"测试集"*（testing set）来测试学习器对新样本的判别能力，然后以测试集上的*"测试误差"*（testing error）作为泛化误差的近似。测试集应该尽可能与训练集**互斥**， 即测试样本尽量不在训练集中出现、未在训练过程中使用过。

假设我们只有一个包含m个样例的数据集$D={(x_1, y_1), (x_2, y_2), …, (x_m, y_m)}$，对其进行适当的处理以产生训练即S 和测试集T。

### 留出法

*"留出法"*（hold-out）直接将数据集D 划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即 $D=S\cup T$， $S\cap T=\emptyset$。在 S 上训练出模型后，用T 来评估其测试误差，作为对泛化误差的估计。

**NOTE**：

- 训练/测试集的划分要尽可能**保持数据分布的一致性**，避免因数据划分过程中引入额外的偏差而对最终结果产生影响。常用的用于保留类别比例的采样方式通常称为*“分层采样”*。
- 使用留出法时，一般采用若干次随机划分、重复进行实验评估后**取平均值**作为留出法的评估结果。

**问题**

- 我们希望模型是用D训练出来的，但留出法需要划分训练/测试集。从*“偏差-方差”*的角度来看：测试集小时，评估结果的方差较大；训练集小时，评估结果的偏差较大。常见做法式将大约2/3 ～ 4/5的样本用于训练，剩余样本用于测试。

### 交叉验证法

*“交叉验证法”*（cross validation）先将数据集D划分为**k个大小相似的互斥子集**，即$D = D_1 \cup D_2 \cup D_3 \cup … \cup D_k, D_i \cap D_j = \emptyset (i \neq j)$. 然后每次用k - 1 个子集的并集作为训练集，余下的那个子集作为测试集。从而进行k 次训练和测试，最终发挥k 次测试结果的均值。也称为*k折交叉验证*（k-fold cross validation）。

![Snip20171215_1](/Users/lipenghua/Downloads/Snip20171215_1.png)

与留出法相似，子集的划分有多种方式。**为了减小因样本划分而引入的误差**，k折交叉验证通常随机使用不同的划分重复p次，称为*p次k折交叉验证*（与p * k 次留出法类似）。

假定数据集D包含m个样本，若令k = m，则得到*”留一法“*（LOO）。

### 自助法

我们希望评估的是用D训练出的模型，而且前两种方法都存在训练样本减少的问题。*”自助法“*是一个比较好的解决方案。

**定义**：给定包含m个样本的数据集D，采样方法为：每次随机挑选一个样本，将**其拷贝**放入数据集D^'^ ，再将至放回D中。重复m次后则得到含有m个样本的数据集D^'^。该方法亦称为*”可重复采样“*或*”有放回采样“*。

显然，有一部分样本会在D^'^中重复出现而有一些不出现。样本在m次采样中始终不被采到的概率为$(1 - \frac{1}{m})^m$，取极限后得到

$$\lim\limits_{m\to\infty} (1 - \frac{1}{m}) ^ m \to \frac{1}{e} \approx 0.368$$

即通过自主采样，D中约有36.8%的样本未出现在D^'^中。此时D^'^为训练集，D\D^'^为测试集。

**适用于**数据集较小，难以有效划分训练/测试集。

## 调参与最终模型

在进行模型评估与选择时，除了要对适用学习算法进行选择，还需对算法参数进行设定，这就是通常所说的*"参数调节"*（parameter tuning）。

**常用做法**是对每个参数选定一个**范围**和**变化步长**，例如在 [0, 0.2] 范围内以 0.05 为步长。

## 性能度量

*“性能度量”*（performance measure）：衡量模型泛化能力。

要评估学习器f 的性能，就要把学习器的预测结果f(x) 与真实标记y 进行比较。例如回归任务重常用的性能度量是“均方误差”（mean suqared error）

$$E(f;D)=\frac{1}{m} \sum\limits_{i=1}^{m}(f(x_i)-y_i)^2$$

### 错误率与精度

对于样例集D，分类错误率为

$$E(f;D)=\frac{1}{m}\sum\limits_{i=1}^{m} \Pi(f(x_i) \neq y_i)$$

精度则为

$$acc(f;D)=\frac{1}{m} \sum\limits_{i=1}^{m} \Pi(f(x_i)=y_i)=1-E(f;D)$$

### 查准率、查全率与F1

将样例根据其真实类别与学习器预测类别的组合划分为*真正例* (true positive) 、*假正例* (false positive) 、*真反例* (true negative) 、 *假反例* (false negative) 四种情形。咳得混淆矩阵：

![Snip20171215_2](/Users/lipenghua/Downloads/Snip20171215_2.png)

*查全率* R（recall）和*查准率* P（precision）分别定义为

$$P=\frac{TP}{TP+FP}$$

$$R=\frac{TP}{TP+FN}$$

查准率和查全率是一对**矛盾**的度量，只有在简单任务中二者才会都很高。

在很多情形札我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为"最可能 "是正例的样本。按此顺序逐个把样本作为正例进行预测 ，则每次可以计算出当前的查全率、查准率。**以查准率为纵轴、查全率为横轴作图** ，就得到了*查准率-查全率曲线*，简称 *"P-R曲线"*。

![Snip20171215_3](/Users/lipenghua/Downloads/Snip20171215_3.png)

**比较**：

- 若一个学习器的 P-R 曲线被另一个学习器的曲线完全"包住" ， 则可断言后者的性能优于前者；
- 如果两个学习器的 P-R 曲线发生了交叉，这时一个比较合理的判据是比较 P-R 曲线节面积的大小。

**平衡点**（BEP）：查全率=查准率时的取值。

**F1度量**：$F1=\frac{2*P*R}{P+R}=\frac{2*TP}{样例总数+TP-TN}$. F1度量时查全率和查准率的**调和平均**：$\frac{1}{F1}=\frac{1}{2} (\frac{1}{P}+\frac{1}{R})$.

如果对查全率和查准率的重视程度有所不同，则可以使用$F_\beta$度量：$F_\beta =\frac{(1+\beta ^2) * P * R}{(\beta ^2 * P) + R}$，即**加权调和平均**：$\frac{1}{F_\beta}=\frac{1}{1+F_\beta}(\frac{1}{P}+\frac{\beta ^2}{R})$。其中$\beta>0$度量了**查全率对查准率的相对重要性**：$\beta>1$，查全率更重要；$\beta<1$，查准率更重要。

若进行多次训练，则会得到**多个二分类混淆矩阵**，则

1. 先计算各个混淆矩阵的查准率和查全率，再计算平均值。就得到*“宏查准率*”（macro-P）、*“宏查全率”*（macro-R）以及相应的*”宏F1“*（macro-F1）：

   $$macro \text{-}P=\frac{1}{n} \sum\limits_{i=1}^{n}P_i$$

   $$macro \text{-}R=\frac{1}{n} \sum\limits_{i=1}^{n}R_i$$

   $$macro\text{-}F1=\frac{2*macro-P*macro_R}{macro-P+macro-R}$$

2. 先将各混淆矩阵的对应元素进行平局，得到TP, FN, TN, FN 的平均值，再计算出*“微查准率*”（micro-P）、*“微查全率”*（micro-R）以及相应的*”微F1“*（micro-F1）

   $$micro\text{-}P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$

   $$macro\text{-}R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$

   $$macro\text{-}F1=\frac{2*micro-P*micro_R}{micro-P+micro-R}$$

### ROC和AUC

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与 一个*分类阔值*(threshold)进行比较，若大于阈值值则分为正类，否则为反类。

实际上，根据这个实值或概率预测结果，我们可将测试样本进行**排序**，"最可能"是正例的排在最前面， "最不可能"是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个*"截断点"* (cut point)将样本分为两部分，前一部分判作正例，后一部分则判作反例。若更重视"查准率"，则可选择排序中靠前的位置进行截断；若更重视"查全率"，则可选择靠后的位置进行截断。

**定义**：*ROC* 全称是"受试者工作特征" (Receiver Operating Characteristic) 曲线。与 P-R 曲线相似，根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、纵坐标作图'就得到了 "ROC 曲线“。ROC 曲线的纵轴是*"真正例率"* ($TPR=\frac{TP}{TP+FN}$，**同查全率**) ，横轴是 *“假正例率”* ($FPR=\frac{FP}{TN+FP}$)。

![Snip20171215_5](/Users/lipenghua/Downloads/Snip20171215_5.png)

显然，对角线对应于*”随机猜测“*；点（0, 1）对应于**把所有正例排在所有反例之前**的*”理想模型“*。

**绘图过程**：:给定 m+ 个正例和m- 个反例，根据学习器预测结果对样例进行排序，然后把分类阔值设为最大，即**把所有样例均预测为反例**，此时真正例率和假正例率均为 0， 在坐标 (0, 0) 处标记一个点然后，**将分类阐值依次设为每个样例的预测值**，即依次将每个样例划分为正例.设前一个标记点坐标为 (x, y) ，当前若为真正例，则对应标记点的坐标为 ($x, y+\frac{1}{m\text{+}}$) ;当前若为假正例，则对应标记点的坐标为 ($x+\frac{1}{m \text{-}}, y$) ，然后用线段连接相邻点即得。

**比较**：

- 若一个学习器的 ROC 曲线被另一个学习器的曲线完全"包住"， 则可断言后者的性能优于前者；

- 若两个学习器的 ROC 曲线发生交叉，较为合理的判据是比较 ROC 曲线下的面积，即 *AUC* (Area Under ROC Curve)。

  $$AUC=\frac{1}{2} \sum\limits_{i=1}^{m-1}(x_{i+1}-x_i) \cdot (y_i+y_{i+1})$$

形式化地看，AUC 考虑的是样本预测的**排序质量**，因此它与**排序误差**紧密相连。

给定 m+ 个正例和 m- 个反例，令 D+ 和 D- 分别表示正、反例集合，则*排序"损失"* (loss)定义为

$$l_{rank}=\frac{1}{m^+ \cdot m^-} \sum\limits_{x^+ \in D^+} \sum\limits_{x^- \in D^-}(\Pi(f(x^+)<f(x^-) + \frac{1}{2}\Pi(f(x^+)=f(x^-))))$$

即考虑每一对正、反例，若正例的预测值小于反例，则记一个罚分；若相等，则记 0.5 个罚分。容易看出，$l_{rank}$对应的是 ROC 曲线之上的面积：$AUC=1-l_{rank}$.

### 代价敏感错误率与代价曲线

为权衡不同类型错误所造成的不同损失，可为错误赋予*"非均等代价"*(unequal cost)。

则对于一个*“代价矩阵”*（cost matrix），cost~ij~ 表示将第 i 类样本预测为第 j 类样本的代价.一般来说 ，cost~ii~ = 0.

![Snip20171215_6](/Users/lipenghua/Downloads/Snip20171215_6.png)

在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望**最小化"总体代价"** (total cost)。*”代价敏感“*（cost-sensitive）错误率为：

$$E(f;D;cost)=\frac{1}{m}(\sum\limits_{x_i \in D^+} \Pi (f(x_i) \neq y_i) \cdot cost_{01} + \sum\limits_{x_i \in D^-} \Pi (f(x_i) \neq y_i) \cdot cost_{10})$$

在非均等代价下，ROC 曲线不能直接反映出学习器的期望总体代价，而*"代价曲线"* (cost curve) 则可达到该目的。

## 比较检验


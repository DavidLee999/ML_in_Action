# 线性模型

## 基本形式

给定由 *d* 个属性描述的示例 x = (x~1~; x~2~; … ;x~d~), 其中 x~i~ 是 x 在第 i 个属性上的取值，线性模型(linear model) 试图学得一个通过属性的**线性组合**来进行预测的函数：
$$
f(\mathbf{x}) = \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d + b
$$
向量形式：
$$
f(\mathbf{x})=\mathbf{\omega}^T \cdot \mathbf{x}+b
$$
在 $\mathbf{\omega}$ 和 $b$ 学得后，模型就得以确定。

**优点**：

- 线性模型形式简单、易于建模；
- 许多功能更为强大的非线性模型 (nonlinear model) 可在线性模型的基础上，通过引入**层级结构**或**高维映射**而得；
- 由于 $\mathbf{\omega}$ 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性(comprehensibility)。

## 线性回归

**定义**：给定数据集 $D=\{ (\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), …, (\mathbf{x_3}, y_3)\}$，"线性回归" (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.

**NOTE**：对于离散属性，若属性值间存在"序" (order)关系，可通过连续化将其转化为连续值；若属性值间不存在序关 系，假定有 k 个属性值，则通常转化为 k 维二值向量。

**目标**：$f(x_i)=\omega_ix_i+b$ 使得 $f(x_i) \approx y_i$

而确定 $\omega$ 和 $b$ 的**关键**在于衡量 f(x) 和 y 之间的差别。*均方误差*是回归任务中最常用的性能度量。均方误差具有很好的几何意义，对应于*欧式距离*。基于均方误差来进行模型求解的方法称为*最小二乘法*。

设$E_{w,b}=\sum_{i=1}^{m}(y_i-\omega x_i -b)$. 将之分别对 $\omega$ 和 $b$ 求导并令等式为零即可得到结果。

> 这里$E_{w,b}$ 是关于 $\omega$ 和 $b$ 的凸函数。对区间 [a, b] 上定义的函数 f，若它对区间中任意两点 x~1~, x~2~ 均有 $f(\frac{x_1+x_2}{2}) \le \frac{f(x_1)+f(x_2)}{2}$，则称 f 为区间 [a, b] 上的**凸函数**。对于实数集上的函数，可通过**求二阶导数**来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于零则称为严格凸函数。

对于多元线性回归，设 $\hat{\mathbf{\omega}}=(\mathbf{\omega}; b)$, $\mathbf{y}=(y_1;y_2; … ; y_m)$ 而数据集表示为

$$\mathbf{X}=\begin{bmatrix} \mathbf{x}_1^T & 1 \\ \mathbf{x}_2^T & 1 \\ \vdots & \vdots \\ \mathbf{x}_m^T & 1 \end{bmatrix}$$

则有
$$
\hat \omega^*=\mathop{\arg \min}_{\hat \omega} (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})
$$
令$E_\hat\omega=(\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})$ 并对 $\hat\omega$ 求导可得到：
$$
\frac{\partial E_\hat\omega}{\partial \hat\omega}=2\mathbf{X}^T (\mathbf{X} \hat\omega-\mathbf{y})
$$
令上式为零即可得到 $\hat\omega$ 最优解的闭式解。当$\mathbf{X}^T \mathbf{X}$为满秩矩阵，则可得
$$
\hat\omega^*=(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
则线性回归的模型为
$$
f(\hat x_i)=\hat x_i^T(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
对于非满秩的情况，最常见的接发则是引入正则化（regularization）项。

线性模型虽简单，却有丰富的变化。譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标， 即
$$
ln\ y=\mathbf{\omega}^T \mathbf{x}+b
$$
这就是*对数线性回归*（log-linear regression）。

更一般地，考虑单调可微函数 g，令 $y=g^{-1}(\mathbf{\omega}^T \mathbf{x} + b)$. 即可得到*广义线性模型*（generalized linear model）。而 g 称为*联系函数*（link function）。

## 对数几率回归

前一节讨论了利用线性模型进行回归学习，若要进行分类任务，则可利用广义线性模型。

考虑二分类任务，